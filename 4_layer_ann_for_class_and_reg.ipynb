{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prhKha9bGLDf"
      },
      "source": [
        "  **GOAL** : Create a neural network with at least one hidden layer to model\n",
        "a) California housing dataset (regression),\n",
        "b) Full MNIST dataset (classification)\n",
        "Use sklearn packages.\n",
        "Submit python files and also report the RMSE for regression and accuracy for classification on the test sets. Choose random_state  = 42 for splitting the dataset of california housing into 75% for training and 25% for testing, the first 60k samples for train, and remaining for test in Full MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "i have created a 4 layer neural network with two hidden layers. activation function used in hidden layer is ReLU."
      ],
      "metadata": {
        "id": "mQijJ_YGDPn4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdvhUcbh3x4m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.neural_network import MLPRegressor,MLPClassifier\n",
        "from sklearn.metrics import mean_squared_error,accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFbR46Rri3B4"
      },
      "outputs": [],
      "source": [
        "def ReLU(z):\n",
        "  return np.maximum(0,z)\n",
        "def ReLUD(z):\n",
        "  return np.heaviside(z,1)\n",
        "def Sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "def Linear(z):\n",
        "  return(z)\n",
        "def Softmax(Z):\n",
        "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "    return expZ / np.sum(expZ, axis=0, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_Y6_YVli9Z-"
      },
      "outputs": [],
      "source": [
        "def forward_prop(W1,W2,W3,X,b1,b2,b3,p):\n",
        "    Z1 = W1 @ X + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2 @ A1 + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = W3 @ A2 + b3\n",
        "    if p==1:\n",
        "      A3=Linear(Z3)\n",
        "    elif p==2:\n",
        "      A3=Softmax(Z3)\n",
        "    return Z1,A1,Z2,A2,Z3,A3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwkrJ8nUjASX"
      },
      "outputs": [],
      "source": [
        "def backward_prop(Y,A3,A2,A1,W3,W2,X,Z2,Z1,p):\n",
        "  m=X.shape[1]\n",
        "  if p==1:\n",
        "    DZ3=2*(A3-Y)/m\n",
        "    DW3=DZ3 @ A2.T\n",
        "    Db3=np.sum(DZ3,axis=1,keepdims=True)\n",
        "\n",
        "    DZ2=(W3.T @ DZ3) * ReLUD(Z2)\n",
        "    DW2=DZ2 @ A1.T\n",
        "    Db2=np.sum(DZ2,axis=1,keepdims=True)\n",
        "\n",
        "    DZ1=(W2.T @ DZ2) * ReLUD(Z1)\n",
        "    DW1=DZ1 @ X.T\n",
        "    Db1=np.sum(DZ1,axis=1,keepdims=True)\n",
        "\n",
        "  elif p==2:\n",
        "    DZ3=(A3 - Y)\n",
        "    DW3=(1/m) * DZ3 @ A2.T\n",
        "    Db3=(1/m) * np.sum(DZ3, axis=1, keepdims=True)\n",
        "\n",
        "    DZ2=(W3.T @ DZ3) * ReLUD(Z2)\n",
        "    DW2=(1/m) * DZ2 @ A1.T\n",
        "    Db2=(1/m) * np.sum(DZ2, axis=1, keepdims=True)\n",
        "\n",
        "    DZ1=(W2.T @ DZ2) * ReLUD(Z1)\n",
        "    DW1=(1/m) * DZ1 @ X.T\n",
        "    Db1=(1/m) * np.sum(DZ1, axis=1, keepdims=True)\n",
        "\n",
        "  return DW3,DW2,DW1,Db3,Db2,Db1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqInNMQsjDGy"
      },
      "outputs": [],
      "source": [
        "def cost_func_cross_entrop(Yhat,Y):\n",
        "  m=Yhat.shape[1]\n",
        "  ep=1e-14\n",
        "  loss=-Y*np.log(np.maximum(Yhat,ep))-(1-Y)*np.log(np.maximum((1-Yhat),ep))\n",
        "  return np.sum(loss,axis=1)/m\n",
        "def cost_func_MSE(Yhat, Y):\n",
        "    m = Y.shape[1]\n",
        "    loss = np.sum((Yhat - Y) ** 2) / m\n",
        "    return loss\n",
        "def cost_func_cat_cross_entrop(Yhat, Y):\n",
        "    m = Y.shape[1]\n",
        "    eps = 1e-12\n",
        "    cost = -np.sum(Y * np.log(np.maximum(Yhat, eps))) / m\n",
        "    return cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpWwTwC35KFE"
      },
      "source": [
        "**California Dataset**\n",
        "\n",
        "This is a regression problem, so i have used sigmoid activation function at the output layer.since data is less compared to MNIST i have used batch gradient descent method.in order to avoid vanishing/exploding gradients i have used HE initialization for weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJbHSglN5JQO",
        "outputId": "36923271-3816-491e-bdcd-06bd42b06063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.603140039116418\n",
            "0.45120819215467217\n",
            "0.4233309564673106\n",
            "0.4029618848585914\n",
            "0.3862686975150037\n",
            "0.3743143031340625\n",
            "0.36531593296941817\n",
            "0.35745589916807147\n",
            "0.3508164755368316\n",
            "0.3445552019340799\n",
            "0.33888910618280166\n",
            "0.33375529397394943\n",
            "0.3291138942804739\n",
            "0.3245328203134036\n",
            "0.32008428202843436\n",
            "0.3158823425575742\n",
            "0.31221242044385183\n",
            "0.3089433988340376\n",
            "0.31361450933403084\n",
            "0.310183386346048\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X, y = datasets.fetch_california_housing(return_X_y=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.25,random_state=42)\n",
        "X_train_scaled = scaler.fit_transform(X_train).T\n",
        "X_test_scaled = scaler.transform(X_test).T\n",
        "m_test=X_test.shape[0]\n",
        "m_train=X_train.shape[0]\n",
        "Y=Y_train.reshape(1,m_train)\n",
        "Ytest=Y_test.reshape(1,m_test)\n",
        "\n",
        "np.random.seed(42)\n",
        "n1,n2,n3,n4=8,60,30,1\n",
        "W1=np.random.randn(n2,n1)*np.sqrt(1/(n1))\n",
        "W2=np.random.randn(n3,n2)*np.sqrt(1/(n2))\n",
        "W3=np.random.randn(n4,n3)*np.sqrt(1/(n3))\n",
        "b1=np.random.randn(n2,1)\n",
        "b2=np.random.randn(n3,1)\n",
        "b3=np.random.randn(n4,1)\n",
        "\n",
        "itrations=10000\n",
        "alpha=0.01\n",
        "\n",
        "for i in range(itrations):\n",
        "  Z1,A1,Z2,A2,Z3,Yhat=forward_prop(W1,W2,W3,X_train_scaled,b1,b2,b3,1)\n",
        "  DW3,DW2,DW1,Db3,Db2,Db1=backward_prop(Y,Yhat,A2,A1,W3,W2,X_train_scaled,Z2,Z1,1)\n",
        "  W1=W1-alpha*DW1\n",
        "  W2=W2-alpha*DW2\n",
        "  W3=W3-alpha*DW3\n",
        "  b1=b1-alpha*Db1\n",
        "  b2=b2-alpha*Db2\n",
        "  b3=b3-alpha*Db3\n",
        "  cost=cost_func_MSE(Yhat,Y)\n",
        "  if i%500==0:\n",
        "    print(cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07mvzEA4xKI9",
        "outputId": "9e5f1835-502d-4066-aba8-adbb4619d99f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 0.5671816175900567\n"
          ]
        }
      ],
      "source": [
        "Z1,A1,Z2,A2,Z3,Yhat_test=forward_prop(W1,W2,W3,X_test_scaled,b1,b2,b3,1)\n",
        "rmse = np.sqrt(mean_squared_error(Ytest.flatten(), Yhat_test.flatten()))\n",
        "print(\"Test RMSE:\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Mini-batch generator -----------------\n",
        "def get_batches(X, Y, batch_size):\n",
        "    m = X.shape[1]\n",
        "    idx = np.random.permutation(m)\n",
        "    for i in range(0, m, batch_size):\n",
        "        batch = idx[i:i+batch_size]\n",
        "        yield X[:, batch], Y[:, batch]\n",
        "\n",
        "# ----------------- Accuracy -----------------\n",
        "def accuracy(X, Y, W1, b1, W2, b2, W3, b3):\n",
        "    _, _, _, _, _, A3 = forward_prop(W1,W2,W3,X,b1,b2,b3,2)\n",
        "    preds = np.argmax(A3, axis=0)\n",
        "    labels = np.argmax(Y, axis=0)\n",
        "    return np.mean(preds == labels) * 100"
      ],
      "metadata": {
        "id": "ajxLOyZ44tZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXzLwhT3839h"
      },
      "source": [
        "**MNIST datasets**\n",
        "\n",
        "since it is a multiclass classification problem i have used softmax activation function at the output layer.i converted 28x28 pixels to 784 feature vector, so in input layer i have to take 784 neurons and since i am categorizing 10 digits my output layer is also contains 10 neurons.onehot encoding is used.since data is huge i have used mini batch gradient descent for faster computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmYFV8wwC8Tm",
        "outputId": "523b2872-8ed7-4198-e053-03fceb844d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Cost: 1.0438\n",
            "Epoch 2/20 | Cost: 0.4469\n",
            "Epoch 3/20 | Cost: 0.3448\n",
            "Epoch 4/20 | Cost: 0.2944\n",
            "Epoch 5/20 | Cost: 0.2619\n",
            "Epoch 6/20 | Cost: 0.2385\n",
            "Epoch 7/20 | Cost: 0.2202\n",
            "Epoch 8/20 | Cost: 0.2054\n",
            "Epoch 9/20 | Cost: 0.1933\n",
            "Epoch 10/20 | Cost: 0.1826\n",
            "Epoch 11/20 | Cost: 0.1740\n",
            "Epoch 12/20 | Cost: 0.1656\n",
            "Epoch 13/20 | Cost: 0.1579\n",
            "Epoch 14/20 | Cost: 0.1516\n",
            "Epoch 15/20 | Cost: 0.1457\n",
            "Epoch 16/20 | Cost: 0.1397\n",
            "Epoch 17/20 | Cost: 0.1348\n",
            "Epoch 18/20 | Cost: 0.1299\n",
            "Epoch 19/20 | Cost: 0.1257\n",
            "Epoch 20/20 | Cost: 0.1213\n",
            "  Test Accuracy: 95.37%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(60000, -1) / 255.0\n",
        "X_test = X_test.reshape(10000, -1) / 255.0\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train).T\n",
        "X_test = scaler.transform(X_test).T\n",
        "\n",
        "enc = OneHotEncoder(sparse_output=False)\n",
        "Y_train = enc.fit_transform(y_train.reshape(-1, 1)).T\n",
        "Y_test = enc.transform(y_test.reshape(-1, 1)).T\n",
        "\n",
        "np.random.seed(42)\n",
        "n_x, n_h1, n_h2, n_y = 784, 240, 120, 10\n",
        "W1 = np.random.randn(n_h1, n_x) * np.sqrt(2. / n_x)\n",
        "b1 = np.zeros((n_h1, 1))\n",
        "W2 = np.random.randn(n_h2, n_h1) * np.sqrt(2. / n_h1)\n",
        "b2 = np.zeros((n_h2, 1))\n",
        "W3 = np.random.randn(n_y, n_h2) * np.sqrt(2. / n_h2)\n",
        "b3 = np.zeros((n_y, 1))\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 256\n",
        "alpha = 0.01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_cost = 0\n",
        "    for Xb, Yb in get_batches(X_train, Y_train, batch_size):\n",
        "        Z1,A1,Z2,A2,Z3,A3 = forward_prop(W1,W2,W3,Xb,b1,b2,b3,2)\n",
        "        dW3,dW2,dW1,db3,db2,db1 = backward_prop(Yb,A3,A2,A1,W3,W2,Xb,Z2,Z1,2)\n",
        "\n",
        "        W1 -= alpha * dW1\n",
        "        b1 -= alpha * db1\n",
        "        W2 -= alpha * dW2\n",
        "        b2 -= alpha * db2\n",
        "        W3 -= alpha * dW3\n",
        "        b3 -= alpha * db3\n",
        "\n",
        "        epoch_cost += cost_func_cat_cross_entrop(A3, Yb)\n",
        "\n",
        "    avg_cost = epoch_cost / (X_train.shape[1] // batch_size)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Cost: {avg_cost:.4f}\")\n",
        "acc = accuracy(X_test, Y_test, W1, b1, W2, b2, W3, b3)\n",
        "print(f\"  Test Accuracy: {acc:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}